{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gradio\\components\\image.py:134: UserWarning: The `mirror_webcam` parameter is deprecated. Please use the `webcam_options` parameter with a `gr.WebcamOptions` instance instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLICK_SEND_GPT 아래는 사진 속에서 바운딩 박스로 감지된 물체와 감지 확률, 그리고 자세한 설명입니다.\n",
      "\n",
      "1. 왼쪽 위 (감지확률: 58.04%)\n",
      "   - 감지된 물체: 고양이(새끼 고양이)\n",
      "   - 설명: 귀여운 새끼 고양이가 앞을 바라보고 앉아 있는 모습입니다. 흰색과 회색 줄무늬가 있으며, 똑바로 앉아 카메라를 응시하고 있습니다.\n",
      "\n",
      "2. 가운데 위 (감지확률: 81.26%)\n",
      "   - 감지된 물체: 고양이\n",
      "   - 설명: 갈색 고양이가 뒤돌아 앉아 바깥 풍경을 바라보고 있습니다. 뒷모습이 주로 보이며, 자연 환경 속에 있는 모습입니다.\n",
      "\n",
      "3. 왼쪽 아래 (감지확률: 93.66%)\n",
      "   - 감지된 물체: 고양이\n",
      "   - 설명: 회색 털의 성묘 고양이가 정면을 바라보고 있습니다. 얼굴이 크게 클로즈업되어 있으며, 눈이 매우 또렷하게 보입니다.\n",
      "\n",
      "4. 오른쪽 아래 (감지확률: 51.41%)\n",
      "   - 감지된 물체: 고양이\n",
      "   - 설명: 주황색 줄무늬 고양이가 바닥에 길게 누워서 쉬고 있습니다. 앞발을 쭉 뻗고, 편안한 자세를 취하고 있습니다.\n",
      "\n",
      "이상 네 마리 고양이가 바운딩 박스 안에서 감지되었습니다.\n",
      "CLICK_SEND_GPT 이 사진에서 YOLO 모델이 감지한 물체는 다음과 같습니다:\n",
      "\n",
      "1. 고양이 (cat) - 감지 확률: 96.77%  \n",
      "   - 위치: 사진 왼쪽 중단, 바운딩 박스 안에 있는 갈색과 흰색 털을 가진 어린 고양이입니다. 귀가 크고 눈이 동그랗게 떠 있으며, 몸집이 작고 귀여운 새끼 고양이로 보입니다.\n",
      "\n",
      "2. 고양이 (cat) - 감지 확률: 99.07%  \n",
      "   - 위치: 사진 중앙 중단, 바운딩 박스 안에 있는 회색과 흰색 털을 가진 어린 고양이입니다. 털이 풍성하고 얼굴이 둥글며, 호기심 많은 표정을 짓고 있습니다.\n",
      "\n",
      "3. 고양이 (cat) - 감지 확률: 89.56%  \n",
      "   - 위치: 사진 왼쪽 하단, 바운딩 박스 안에 있는 회색 줄무늬의 새끼 고양이입니다. 귀가 크고 눈이 크며, 앞발을 모으고 앉아 있는 모습입니다.\n",
      "\n",
      "이상 세 개의 바운딩 박스 안에 있는 고양이들만 감지되었습니다. 각각 모두 새끼 고양이로 보이며, 다양한 털 색과 무늬를 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import requests\n",
    "import io\n",
    "import base64\n",
    "import datetime\n",
    "\n",
    "######################################\n",
    "# Azure 관련 전역 변수\n",
    "######################################\n",
    " \n",
    "OPENAI_ENDPOINT = \"https://fimtrus-ai-project-resource.cognitiveservices.azure.com/\"    \n",
    "OPENAI_API_KEY = \"9VEZmMXEZd4aWzFIEVBKCse1NEa3en2LrD51oEyXbFc41XDbcP2VJQQJ99BFACYeBjFXJ3w3AAAAACOGydeD\"\n",
    "DEPLOYMENT_NAME = \"fimtrus-gpt-41\"\n",
    " \n",
    "SPEECH_ENDPOINT = \"https://eastus.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "SPEECH_API_KEY = \"45SFNywqabNccYBIp1fGDsJKJYe5N1pfPZLrSlLGY1ebDzyPLT75JQQJ99BFACYeBjFXJ3w3AAAYACOGWzIo\"\n",
    "\n",
    "\n",
    "weights_path = \"yolo/yolov3.weights\"\n",
    "config_path = \"yolo/yolov3.cfg\"\n",
    "names_path = \"yolo/coco.names\"\n",
    "\n",
    "\n",
    "with open(names_path, 'r', encoding='utf-8') as file:\n",
    "    label_list = file.read().strip().split(\"\\n\")\n",
    "\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "\n",
    "def request_gpt(image_array):\n",
    "\n",
    "    # PIL 형태의 이미지\n",
    "    image = Image.fromarray(image_array)\n",
    "\n",
    "    byte_image = io.BytesIO()\n",
    "    image.save(byte_image, format=\"png\")\n",
    "    base64_image = base64.b64encode(byte_image.getvalue()).decode('utf-8')\n",
    "\n",
    "    # TODO: buffered_io to base64\n",
    "    endpoint = \"{}openai/deployments/{}/chat/completions?api-version=2025-01-01-preview\".format(OPENAI_ENDPOINT, DEPLOYMENT_NAME)\n",
    " \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer {}\".format(OPENAI_API_KEY)\n",
    "    }\n",
    "\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        너는 사진속에서 감지된 물체를 분석하는 AI 봇이야.\n",
    "                        무조건 분석 결과를 한국어로 답변해줘.\n",
    "                        \"\"\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        너는 물체를 감지하는 YOLO 모델이야.\n",
    "                        이 사진에서 감지된 물체에 대해서 감지확률과 자세한 설명을 붙여줘.\n",
    "                        반드시 감지된 물체, 바운딩 박스 안에 있는 물체에 대해서만 설명해줘.\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_tokens\": 16000\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    response_json = response.json()\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def request_tts(text):\n",
    "    endpoint = SPEECH_ENDPOINT\n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": SPEECH_API_KEY,\n",
    "        \"Content-Type\": \"application/ssml+xml\",\n",
    "        \"X-Microsoft-OutputFormat\": \"riff-8khz-16bit-mono-pcm\"\n",
    "    }\n",
    "    body = f\"\"\"\n",
    "        <speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"http://wwww.w3.org/2001/mstts\" xml:lang=\"ko-KR\">\n",
    "        <voice name=\"ko-KR-SeoHyeonNeural\">\n",
    "            {text}\n",
    "    </voice>\n",
    "    </speak>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers, data=body)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    file_name = \"tts_result_{}.wav\".format(datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "    with open(file_name, \"wb\") as audio_file:\n",
    "        audio_file.write(response.content)\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def random_color():\n",
    "    # 랜덤한 RGB 색상 튜플 반환\n",
    "    import random\n",
    "    return (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) \n",
    "\n",
    "\n",
    "def get_font():\n",
    "    # OS별로 적절한 폰트 객체 반환 (한글 지원)\n",
    "    from PIL import ImageFont\n",
    "    import platform\n",
    "    \n",
    "    font_size = 20\n",
    "    \n",
    "    try:\n",
    "        if platform.system() == \"Windows\":\n",
    "            return ImageFont.truetype(\"malgun.ttf\", font_size)\n",
    "        elif platform.system() == \"Darwin\":  # macOS\n",
    "            return ImageFont.truetype(\"AppleGothic.ttf\", font_size)\n",
    "        else:  # Linux      \n",
    "            return ImageFont.load_default(size=font_size)\n",
    "    except IOError:\n",
    "        # 폰트 파일이 없을 경우 기본 폰트 사용\n",
    "        return ImageFont.load_default(size=font_size)\n",
    "    \n",
    "\n",
    "def detect_object(image_array):\n",
    "\n",
    "    image = Image.fromarray(image_array.copy())\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = get_font()\n",
    "\n",
    "    height, width = image_array.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image_array, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_name_list = net.getLayerNames()\n",
    "    out_layer_list = net.getUnconnectedOutLayersNames()\n",
    "    detection_list = net.forward(out_layer_list)\n",
    "\n",
    "    bounding_box_list = list()\n",
    "    confidence_list = list()\n",
    "    label_index_list = list()\n",
    "    \n",
    "    for prediction_list in detection_list:\n",
    "        color = random_color()\n",
    "        #yolo82, yolo94, yolo 106\n",
    "        for prediction in prediction_list:\n",
    "            score_list = prediction[5:]\n",
    "            label_index = np.argmax(score_list)\n",
    "            confidence = score_list[label_index]\n",
    "\n",
    "            if confidence > 0:\n",
    "                bounding_box = prediction[:4] * np.array([width, height, width, height])\n",
    "                center_x, center_y, w, h = bounding_box.astype('int')\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                # print(x, y, w, h)\n",
    "\n",
    "                bounding_box_list.append([x, y, w, h])\n",
    "                confidence_list.append(confidence)\n",
    "                label_index_list.append(label_index)\n",
    "\n",
    "                # draw.rectangle([(x, y), (x + w, y + h)], outline='red', width=2)\n",
    "\n",
    "    extracted_index_list = cv2.dnn.NMSBoxes(bounding_box_list, confidence_list, 0.5, 0.4)\n",
    "\n",
    "    for extracted_index in extracted_index_list:\n",
    "\n",
    "        x, y, w, h = bounding_box_list[extracted_index]\n",
    "        confidence = confidence_list[extracted_index]\n",
    "        label_index = label_index_list[extracted_index]\n",
    "        label_text = label_list[label_index]\n",
    "\n",
    "        draw.rectangle([(x, y), (x + w, y + h)], outline=color, width=2)\n",
    "        draw.text([x + 5, y + 5], \"{}({:.2f}%)\".format(label_text, confidence * 100), fill=color, font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "\n",
    "    def stream_webcam(image):\n",
    "        result_image = detect_object(image)\n",
    "        return result_image\n",
    "    \n",
    "\n",
    "    def click_capture(image):\n",
    "        if image is None:\n",
    "            raise gr.Error(\"감지된 이미지가 없습니다.\", duration=3)\n",
    "        return image \n",
    "    \n",
    "\n",
    "    def click_send_gpt(image_array, histories):\n",
    "        content = request_gpt(image_array)\n",
    "        print(\"CLICK_SEND_GPT\", content)\n",
    "        now = datetime.datetime.now()\n",
    "        label_text = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "        histories.append({\"role\": \"user\", \"content\": gr.Image(label=label_text, value=image_array)})\n",
    "        histories.append({\"role\": \"assistant\", \"content\":content})\n",
    "        # 오른쪽엔 사진, 왼쪽엔 컨텐츠\n",
    "        return histories\n",
    "    \n",
    "\n",
    "    def change_chatbot(histories):\n",
    "        content = histories[-1]['content']\n",
    "        file_path = request_tts(content)\n",
    "        return file_path\n",
    "\n",
    "    with gr.Row():\n",
    "        webcam_image = gr.Image(label=\"실시간 화면\", sources=\"webcam\", width=480, height=270, mirror_webcam=False)\n",
    "        output_image= gr.Image(label=\"검출 화면\", type=\"pil\", width=480, height=270)\n",
    "        output_capture_image = gr.Image(label=\"이상 징후 캡쳐 화면\", interactive=False, width=480, height=270)\n",
    "\n",
    "    with gr.Row():\n",
    "        capture_button = gr.Button(\"이상 징후 발생\")\n",
    "        send_gpt_button = gr.Button(\"분석\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"감지 내용\", type=\"messages\")\n",
    "    chatbot_audio = gr.Audio(label=\"감지 내용\", interactive=False, autoplay=True)\n",
    "\n",
    "    webcam_image.stream(stream_webcam, inputs=[webcam_image], outputs=[output_image])\n",
    "    capture_button.click(click_capture, inputs=[output_image], outputs=[output_capture_image])\n",
    "    send_gpt_button.click(click_send_gpt, inputs=[output_capture_image, chatbot], outputs=[chatbot])\n",
    "\n",
    "    chatbot.change(change_chatbot, inputs=[chatbot], outputs=[chatbot_audio])\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "# test_image = cv2.imread(\"C:/Users/USER/Downloads/ImageTaggingSample1-fd324157.jpg\")\n",
    "# test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "# request_gpt(test_image)\n",
    "\n",
    "# detect_object(test_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
